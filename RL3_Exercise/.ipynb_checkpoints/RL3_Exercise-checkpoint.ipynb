{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise RL3.1: Bellman equation\n",
    "\n",
    "For the first gridworld discussed in the lecture RL3 write down\n",
    "- the policy\n",
    "- the probability $p(s',r|s,a)$ for s=(1,1), s=(0,2) and s=A.\n",
    "- verify the Bellman equation for state (1,1), state (0,2) and state A. Note that the values shown below are only accurate to one decimal place. \n",
    "\n",
    "State indices are such that the state in the upper left corner is state (0,0) and the state in the upper right corner is state (0,4).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise RL3.2: Compute the values of states using pen & paper \n",
    "\n",
    "![Test](grid_world.png)\n",
    "\n",
    "Consider a 2x4 grid world composed of two rows and four columns. Each of the cells is identified by its row and column index written as (row, column). For example, the cell at the top left corner is given by (0,0). An agent in this grid world resides in one of the cells at each point in time. Hence, its state is given by the cell the agent is in. In each time step, the agent can perform one of four different actions: move LEFT, DOWN, RIGHT or UP. These actions are denoted as 0,1,2 and 3, correspondingly. Each of the actions causes the agent to move one cell toward the direction, if a cell exists at this direction. If not, the agent stays at the current location, hence it remains in the current state. For example, if the state of the agent was at (1,1) and the action was RIGHT (=2), the state will change to (1,2). If the action was DOWN (=1), the agent will stay at the same state, (1,1). Whenever the agent moves one step, it takes effort and the reward of that effort is -1. The goal state is at (1,3) and represented by a blue cell. When the agent arrives at the goal state, the episode finishes and the agent recieves +10 as reward. Note that this is a deterministic envrionment. For the moment assume that there is no discounting (i.e. $\\gamma$=1).\n",
    "\n",
    "Different agents can have different policies to act in this environment. Our objective is to compute the value functions for the different policies. \n",
    "\n",
    "In the following you will be asked to compute value functions for different policies in this gridworld. In this exercise you are asked to do this with pen and paper by using the Bellman and not by using a python program which implements one of the reinforcement learning algorithms (e.g. policy evaluation). In later exercises you will use python implementaions of different reinforcement learning algorithms to study this gridworld further.\n",
    "\n",
    "## Policy 1\n",
    "- Consider the agent's policy as shown below.\n",
    "\n",
    "![Test](policy1.png)\n",
    "\n",
    "- The arrow of each cell represents that whenever the agent is at the cell, it will move toward that direction.\n",
    "- Every action produces a reward of  -1.\n",
    "- Compute the value of each state for this policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Policy 2\n",
    "- Consider the agent's alternative policy as shown below. \n",
    "\n",
    "![Test](policy2.png)\n",
    "\n",
    "- The arrow of each cell represents that whenever the agent is at the cell, it will move toward that direction. \n",
    "- If two arrows are shown inside a cell, it means that the agent will randomly chooose one of the two actions with probability p=0.5 each.\n",
    "- Every action produces a reward of  -1.\n",
    "- Compute the value of each state for this policy.\n",
    "- Which policy (policy 1 or policy 2) would you say is better and why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Policy 3 with state loop\n",
    "\n",
    "- Policy 3 shown below is a policy with a state loop: \n",
    "\n",
    "![Test](policy4_nontrivial.png)\n",
    "\n",
    "- The arrow of each cell represents that whenever the agent is at the cell, it will move toward that direction. \n",
    "- If two arrows are shown inside a cell, it means that the agent will randomly chooose one of the two actions with p=0.5 each.\n",
    "- Compute the value of each state for this policy. \n",
    "- Note, that this is more difficult than as in the previous examples. Can you find the solution?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Which is the optimal policy?\n",
    "\n",
    "Which policies do you think is the best among the three proposed policies, policy 1, policy 2 and policy 3? Do you think any of these policies is optimal and why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Bellman optimality equation\n",
    "\n",
    "Use the Bellman optimality equation to test for policy 1 and policy 3 whether they are optimal or not.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
